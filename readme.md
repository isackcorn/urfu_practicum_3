# Виртуальный помощник в чате со студентами

Телеграм-бот на основе ИИ-технологий, предназначенный для ответов на вопросы по программе «Прикладной искусственный интеллект».
Бот использует технологию RAG (Retrieval-Augmented Generation): сначала ищет релевантную информацию в базе знаний (`csv`), а затем генерирует ответ с помощью локально-развернутой LLM (**Qwen 2.5 3B Instruct**).

---

## Команда
- Лобачев Василий Иванович
- Кипина Анна Владимировна

---

## Запуск и установка

### Вариант 1: Запуск в Docker

Рекомендуемый способ для деплоя (на свервере, например)

#### 1. Сборка образа Docker
В корне проекта выполните:
```
docker build -t ai-bot-rag .
```

#### 2. Запуск контейнера
Запуск в фоновом режиме:
```
docker run -d --name my-ai-bot ai-bot-rag
```

#### 3. Логи и остановка
```
# Посмотреть логи
docker logs -f my-ai-bot

# Остановить бота
docker stop my-ai-bot
```

---

### Вариант 2: Локальный запуск

Этот способ может быть предпочтительным, если вы хотите использовать аппаратное ускорение (Metal/MPS).

#### 1. Подготовка окружения
Откройте терминал в папке проекта:

```
# Создаем виртуальное окружение
python3 -m venv venv

# Активируем его
source venv/bin/activate
```

#### 2. Установка зависимостей
```
pip install -r requirements.txt
```

#### 3. Запуск бота
Перейдите в папку с кодом и запустите скрипт:

```
cd src
python main.py
```
*Скрипт автоматически определит чип Apple M-серии и будет использовать устройство `MPS` для ускорения.*

---

## Стек

*   **LLM:** `Qwen/Qwen2.5-3B-Instruct` — легкая модель (~6 ГБ RAM), отлично работающая на русском языке.
*   **Embeddings:** `sberbank-ai/sbert_large_nlu_ru` — для семантического поиска.
*   **RAG:** Поиск по косинусному сходству (Cosine Similarity).
*   **Язык/Основные библиотеки:** Python 3.10, PyTorch, Transformers, pyTelegramBotAPI.

---

## Принцип работы бота (обобщённо)

Бот построен на архитектуре **RAG (Retrieval-Augmented Generation)** — генерация ответов с контекстом. 

### Алгоритм работы:

1.  **Подготовка:**
    *   При запуске скрипт читает базу знаний (`faq_dataset.csv`).
    *   Все вопросы из базы превращаются в эмбеддинги с помощью модели `sbert_large_nlu_ru`.
    *   Эти векторы сохраняются в файл `faq_vectors.npy`, чтобы не пересчитывать их каждый раз (кэширование).

2.  **Поиск:**
    *   Когда пользователь присылает вопрос, он тоже превращается в вектор.
    *   Бот сравнивает вектор вопроса пользователя со всеми векторами из базы (используя косинусное сходство).
    *   Находятся топ-5 самых похожих вопроса из базы, их **ответы** извлекаются как контекст для LLM.

3.  **Генерация:**
    *   Найденные ответы объединяются в единый текст (контекст).
    *   В LLM (языковую модель `Qwen`) отправляется промпт с контекстом.
    *   Модель генерирует финальный ответ для пользователя бота.

### Расширение базы ответов

При необходимости можно быстро добавить новую информацию в базу знаний путем редактирования `csv`-файла, содержащего поля вида `Вопрос`-`Ответ`.

### Улучшение качества ответов

Если ресурсы ПК/Сервера позволяют, то можно выбрать модель с большим числом параметров из того же семейства, например: [Qwen/Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct). 

Это может существенно повысить качество ответов, скорость генерации при этом снизится. 

Для этого необходимо изменить константу `HF_MODEL_NAME` в `src/main.py`.